{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking_GPU_Availabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMD Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard --logdir=\"C:\\Users\\User\\OneDrive - University College London\\UCL Education\\Year 4\\MLS\\Coursework\\AMLS_assignment24_25\\Task A\\runs\\DisplayImage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports, data loading and visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import BreastMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "from tensorboard import program\n",
    "\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from tqdm import tqdm  # For nice progress bar!\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datatsets from BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomAdjustSharpness(sharpness_factor=2,p=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomAdjustSharpness(p=1,sharpness_factor=1.1),\n",
    "    # transforms.RandomEqualize(p=0.3),\n",
    "    # transforms.RandomVerticalFlip(p=0.1),\n",
    "    # transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.Pad(padding=98, fill=0),  # Zero padding to make the image 224x224\n",
    "    #transforms.Pad(padding=50, fill=0),  # Add zero padding to make 128x128\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Mean and standard deviation for grayscale images\n",
    "])\n",
    "#Cannot blindly augment the data to what we want\n",
    "#the classifiers depend on the chape of the tumer hence all the below is good\n",
    "#Might consider increaasing the contrast and brightness to allow easier identificationn\n",
    "#CANCEL contrast... USE histogram equaliser instetad\n",
    "transforms.RandomVerticalFlip()\n",
    "transforms.RandomHorizontalFlip()\n",
    "transforms.RandomRotation(degrees=180)\n",
    "transforms.RandomEqualize(p=1)\n",
    "transforms.RandomAdjustSharpness(p=1,sharpness_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "#loading train,val,test into variables\n",
    "train_data=medmnist.BreastMNIST(split=\"train\",transform=transform)\n",
    "val_data=medmnist.BreastMNIST(split=\"val\",transform=transform)\n",
    "test_data=medmnist.BreastMNIST(split=\"test\",transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation and class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: Counter({1: 399, 0: 147})\n",
      "Batch 1 class distribution: Counter({0: 17, 1: 15})\n",
      "Batch 2 class distribution: Counter({0: 17, 1: 15})\n",
      "Batch 3 class distribution: Counter({0: 19, 1: 13})\n",
      "Total class distribution in the dataloader: Counter({1: 299, 0: 247})\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = medmnist.BreastMNIST(split=\"train\",transform=transform)\n",
    "\n",
    "# Count class occurrences\n",
    "class_counts = Counter(dataset.labels.reshape(-1))\n",
    "print(f\"Class distribution before oversampling: {class_counts}\")\n",
    "\n",
    "# Compute sample weights: Inverse of class frequencies\n",
    "class_weights = 1.0 / torch.tensor([class_counts[cls] for cls in sorted(class_counts.keys())], dtype=torch.float)\n",
    "sample_weights = torch.tensor([class_weights[label] for label in dataset.labels.reshape(-1)], dtype=torch.float)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoader with sampler\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "train_data=dataloader\n",
    "# Check the distribution of classes in a few batches\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1} class distribution: {Counter(labels.reshape(-1).tolist())}\")\n",
    "    if batch_idx == 2:  # Check only the first 3 batches\n",
    "        break\n",
    "\n",
    "# Initialize a Counter for tracking class counts\n",
    "total_class_counts = Counter()\n",
    "\n",
    "# Iterate through the entire dataloader\n",
    "for data, labels in dataloader:\n",
    "    # Flatten the labels and convert them to a list\n",
    "    total_class_counts.update(labels.reshape(-1).tolist())\n",
    "\n",
    "# Print the total distribution of classes\n",
    "print(f\"Total class distribution in the dataloader: {total_class_counts}\")\n",
    "print(len(dataloader)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 52, 0: 12})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels=next(iter(train_loader))\n",
    "x=labels.view(-1)\n",
    "x=x.tolist()\n",
    "print(x)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Example mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(train_data)\n",
    "# print(\"=====================\")\n",
    "# print(val_data)\n",
    "# print(\"=====================\")\n",
    "# print(test_data)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "#visualising the size of the image and its labels\n",
    "one=0\n",
    "zero=0\n",
    "for image , label in train_data:\n",
    "    # print(image.shape)\n",
    "    # print(label)\n",
    "    if label==0:\n",
    "        zero+=1\n",
    "    else:\n",
    "        one+=1\n",
    "print(one)\n",
    "print(zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying images on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location of tensorboard folder\n",
    "folder=\"runs/DisplayImage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1733867044.DESKTOP-3FC1MTH.29016.68\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_folder(folder)\n",
    "#show using dataset on tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "for index in range(100):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1733866663.DESKTOP-3FC1MTH.29016.63\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SummaryWriter.close of <torch.utils.tensorboard.writer.SummaryWriter object at 0x0000022A861F86A0>>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_folder(folder)\n",
    "#show using dataloader with batches\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # create grid of images\n",
    "    img_grid = torchvision.utils.make_grid(data)\n",
    "    # write to tensorboard\n",
    "    writer.add_image(f\"MNIST Example - image batch \", img_grid,batch_idx)\n",
    "    #print(batch_idx)\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label = train_data[1]\n",
    "image_np = img.squeeze()\n",
    "print(image_np.shape)\n",
    "# Plot the image\n",
    "plt.imshow(image_np,cmap=\"gray\")\n",
    "plt.axis('off')  # Hide the axes for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to copy tensor files save\n",
    "def copy_directory(source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Copies all files and folders from a source directory to a destination directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The path to the source directory.\n",
    "        destination_dir (str): The path to the existing destination directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the destination directory exists\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            print(f\"Error: Destination directory '{destination_dir}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Copy the content of source dir to destination directory\n",
    "        shutil.copytree(source_dir, os.path.join(destination_dir, os.path.basename(source_dir)))\n",
    "        print(f\"Successfully copied the content of '{source_dir}' to '{destination_dir}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "    except shutil.Error as e:\n",
    "        print(f\"Error during copy: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#function to clear tensorboard files\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Iterate through all items in the folder\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        try:\n",
    "            # Remove directories\n",
    "            if os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "            # Remove files\n",
    "            else:\n",
    "                os.remove(item_path)\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")\n",
    "    \n",
    "    print(f\"All contents of the folder '{folder_path}' have been cleared.\")\n",
    "    #location of tensorboard folder\n",
    "folder=\"runs/DisplayImage\"\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\n",
    "        Here we define the layers of the network. We create two fully connected layers\n",
    "\n",
    "        Parameters:\n",
    "            input_size: the size of the input, in this case 784 (28x28)\n",
    "            num_classes: the number of classes we want to predict, in this case 2 (0-1)\n",
    "\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "        # # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # # this case 10.\n",
    "        # self.fc1 = nn.Linear(input_size, 50)\n",
    "        # self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "        self.flatten = nn.Flatten() #flattens the input tensors\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x here is the mnist images and we run it through the network that we created above.\n",
    "        Parameters:\n",
    "            x: mnist images\n",
    "        Returns:\n",
    "            out: the output of the network\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(  #same convolution\n",
    "            in_channels=in_channels,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16, \n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\n",
    "        Here we define the layers of the network. We create two fully connected layers\n",
    "\n",
    "        Parameters:\n",
    "            input_size: the size of the input, in this case 784 (28x28)\n",
    "            num_classes: the number of classes we want to predict, in this case 10 (0-9)\n",
    "\n",
    "        \"\"\"\n",
    "        super(NN2, self).__init__()\n",
    "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # this case 10.\n",
    "        self.flatten = nn.Flatten() #flattens the input tensors\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
    "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
    "        I recommend using nn.functional (F)\n",
    "\n",
    "        Parameters:\n",
    "            x: mnist images\n",
    "\n",
    "        Returns:\n",
    "            out: the output of the network\n",
    "        \"\"\"\n",
    "        x=self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    accuracies=[]\n",
    "    batch_loss=0\n",
    "    for batch, (input_data, class_cat) in enumerate(tqdm(dataloader)):\n",
    "        input_data, class_cat = input_data.to(device), class_cat.to(device)\n",
    "\n",
    "        ## Compute prediction error\n",
    "        pred = model(input_data)\n",
    "        class_cat=class_cat.squeeze().long()\n",
    "        loss = loss_fn(pred, class_cat)\n",
    "\n",
    "        ## Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## calculation running loss\n",
    "        loss, current = loss.item(), (batch + 1) * len(input_data)\n",
    "\n",
    "\n",
    "        ##caculating running accuracy\n",
    "        _, predictions = pred.max(1)\n",
    "        num_correct = (predictions == class_cat).sum()\n",
    "        running_train_acc = float(num_correct) / float(input_data.shape[0])\n",
    "        # print(\"model Output>>>>>\")\n",
    "        # print(pred)\n",
    "        # print(\"predictions>>>\")\n",
    "        # print(predictions)\n",
    "        # print(\"num_correct>>>>>\")\n",
    "        # print(num_correct)\n",
    "        # print(\"accuracy>>>>\")\n",
    "        # print(running_train_acc)\n",
    "        # print(data.shape[0])\n",
    "        # print(input_data.shape[0])\n",
    "        accuracies.append(running_train_acc)\n",
    "\n",
    "        ##Plot stuff to tensorboard tensorboard\n",
    "        global step\n",
    "        writer.add_scalar(\"Batch/Training loss\",loss,global_step=step)\n",
    "        writer.add_scalar(\"Batch/Training Accuracy\", running_train_acc, global_step=step)\n",
    "        # global batch_loss\n",
    "        # batch_loss.append(loss)\n",
    "        batch_loss+=loss\n",
    "\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "\n",
    "        #print(f\"loss: {loss:>7f} accuracy: {running_train_acc:>5f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    ## Calculate epoch accuracy\n",
    "    epoch_accuracy=sum(accuracies)/len(accuracies)\n",
    "\n",
    "    ## Getting the average epoch loss\n",
    "    epoch_loss=batch_loss/size\n",
    "    \n",
    "    ## Send it to tensorboard\n",
    "    writer.add_scalars(\"Epoch/Epoch loss\",{'Train':epoch_loss},global_step=epoch)\n",
    "    writer.add_scalars(\"Epoch/Epoch Accuracy\",{\"Train\":epoch_accuracy},global_step=epoch)\n",
    "    \n",
    "def val(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y=y.squeeze().long()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    writer.add_scalars(\"Epoch/Epoch loss\",{'Val':test_loss},global_step=epoch)\n",
    "    writer.add_scalars(\"Epoch/Epoch Accuracy\",{\"Val\":correct},global_step=epoch)\n",
    "    print(f\"val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    val_acc=correct\n",
    "    # Save the model if it's the best so far\n",
    "    global best_val_acc\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model_weights.pth\")\n",
    "        print(f\"New best model saved with accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.squeeze().long()\n",
    "            pred = model(X)\n",
    "            # Collect predictions and true labels\n",
    "            all_labels.append(int(y))\n",
    "            all_predictions.append(int(pred.argmax(1)))\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct /= size\n",
    "\n",
    "    # Build confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_names = [\"False\", \"Positive\"]  # Update as needed for your use case\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()  # Ensure labels fit within figure boundaries\n",
    "    \n",
    "\n",
    "    # Convert plot to image\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.draw()\n",
    "    width, height = fig.canvas.get_width_height()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "    # Log image to \n",
    "    writer.add_image(\"Confusion Matrix\", np.transpose(image, (2, 0, 1)),global_step=1)\n",
    " \n",
    "    # Log accuracy to TensorBoard\n",
    "    writer.add_scalar(\"Test Accuracy\", correct,global_step=1)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%\\n\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "def savemodel(model_name,model,tensor_path):\n",
    "\n",
    "    #Create folder for new model in saved model\n",
    "    try:\n",
    "        os.mkdir(f\"Saved_models/{model_name}\") # Will not create parent folders, unlike os.makedirs()\n",
    "        print(f\"Folder created successfully\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating folder: {e}\")\n",
    "\n",
    "    #Copy tensorboard data of the session to     \n",
    "    tensorpath=\"runs\\DisplayImage\"\n",
    "    copy_directory(tensorpath,f\"Saved_models/{model_name}\")\n",
    "\n",
    "    # 7. Save entire model (Less recommended).\n",
    "    save_entire_model_path = f\"Saved_models/{model_name}/{model_name}.pth\"\n",
    "    torch.save(model, save_entire_model_path)\n",
    "    print(f\"Entire model saved to: {save_entire_model_path}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "#basinc testing for the model\n",
    "model=CNN()\n",
    "x=torch.randn(20,1,28,28)\n",
    "print(model(x).shape)\n",
    "\n",
    "model=NN(input_size=28*28,num_classes=2)\n",
    "x=torch.randn(20,1,28,28)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#batch_size = 64 Note: this was determined when we loaded the data previously\u001b[39;00m\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m(input_size\u001b[38;5;241m=\u001b[39minput_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m## Setting up training and test function\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NN' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "#batch_size = 64 Note: this was determined when we loaded the data previously\n",
    "num_epochs = 100\n",
    "\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "for index, (data,label) in enumerate(train_loader):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "step=0\n",
    "best_val_acc=0.0\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(val_loader, model, loss_fn)\n",
    "    \n",
    "model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n",
    "test(test_loader,model)\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=2, bias=True)\n",
      ")\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1734132046.DESKTOP-3FC1MTH.14348.10\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n",
      "Model sent to tensorboard\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 78.26it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m     train(val_loader, model, loss_fn, optimizer)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# epoch_loss.append(sum(batch_loss)/len(batch_loss))\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest val_accuracy after training=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 232\u001b[0m, in \u001b[0;36mval\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m    230\u001b[0m         y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m    231\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m--> 232\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    233\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    234\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "# Full implementation\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "# for index, (data,label) in enumerate(train_loader):\n",
    "#     data,label=train_data[index]\n",
    "#     writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "step=0\n",
    "best_val_acc=0.0\n",
    "# epoch_loss=[]\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(val_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(test_loader, model, loss_fn)\n",
    "\n",
    "print(f\"Best val_accuracy after training={best_val_acc} \")\n",
    "model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n",
    "test(test_loader,model)\n",
    "val(test_loader, model, loss_fn)\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretained Model RESNET 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Val: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Val'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Val: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Val'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1734323905.DESKTOP-3FC1MTH.27576.0\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n",
      "Model sent to tensorboard\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.959782 \n",
      "\n",
      "New best model saved with accuracy: 0.7308\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.630883 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.623149 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.645221 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.628958 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.612389 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.593265 \n",
      "\n",
      "New best model saved with accuracy: 0.8333\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.541169 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.502692 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 26.9%, Avg loss: 4.564092 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.483021 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.488180 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 28.2%, Avg loss: 3.646185 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.602576 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.878495 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 34.6%, Avg loss: 3.606455 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.985010 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 76.9%, Avg loss: 1.064978 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 7.729010 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.422469 \n",
      "\n",
      "New best model saved with accuracy: 0.9103\n",
      "Epoch 21\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.389881 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.218540 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.918792 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.493045 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.584002 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 41.0%, Avg loss: 3.067231 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.283880 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.025599 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.461449 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.605498 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.672475 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 1.547933 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.409935 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.251066 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.037191 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.503529 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.496368 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.217718 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.942351 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.711843 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 59.0%, Avg loss: 2.487421 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 7.743371 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 20.232781 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 75.6%, Avg loss: 5.343590 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 6.777423 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.671466 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.465521 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.175522 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 9.186934 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 9.473502 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.785315 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.338252 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.500103 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.763946 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.014106 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 48.7%, Avg loss: 3.236810 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.231265 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 76.9%, Avg loss: 3.769171 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.540468 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.491534 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.611590 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 1.238125 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.081310 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.146232 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 2.069160 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.020863 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.219998 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 37.2%, Avg loss: 5.785538 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.718206 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 1.700851 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 3.820722 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.512543 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.607887 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.548710 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.676370 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.619438 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 1.525598 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.232931 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 2.266382 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 2.710582 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 33.3%, Avg loss: 6.423487 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 50.0%, Avg loss: 3.026955 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 3.423847 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.618803 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.790117 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 1.345686 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.791891 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.846296 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 43.6%, Avg loss: 3.949805 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 2.733087 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 3.261736 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 33.3%, Avg loss: 6.548624 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.022397 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.685444 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.708596 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.703829 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.913752 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 1.009301 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.887721 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27576\\672142558.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.769859 \n",
      "\n",
      "Best val_accuracy after training=0.9102564102564102 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27576\\3990433603.py:287: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 84.6%\n",
      "\n",
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.422469 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "# # Freeze the model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad=False\n",
    "\n",
    "model.conv1 = nn.Conv2d(\n",
    "    in_channels=1,  # Change to 1 channel for grayscale\n",
    "    out_channels=64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(2, 2),\n",
    "    padding=(3, 3),\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# # Optionally copy pretrained weights and average across RGB channels\n",
    "# with torch.no_grad():\n",
    "#     pretrained_weights = model.conv1.weight  # Original weights for RGB\n",
    "#     model.conv1.weight.copy_(torch.mean(pretrained_weights, dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "\n",
    "# Adjust the fully connected layer to match the number of output classes\n",
    "num_classes = 2  # Example: MNIST has 10 classes\n",
    "model.fc = nn.Sequential(\n",
    "            nn.Linear(25088, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2),\n",
    ")\n",
    "model.avgpool = nn.Identity()\n",
    "\n",
    "print(model)\n",
    "\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Move model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "for index, (data,label) in enumerate(train_loader):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "\n",
    "step=0\n",
    "# epoch_loss=[]\n",
    "best_val_acc=0.0\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(val_loader, model, loss_fn)\n",
    "\n",
    "print(f\"Best val_accuracy after training={best_val_acc} \")\n",
    "model.load_state_dict(torch.load(\"best_model_weights.pth\"))\n",
    "test(test_loader,model)\n",
    "val(val_loader, model, loss_fn)\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save current model with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating folder: [WinError 183] Cannot create a file when that file already exists: 'Saved_models/R18_E&E'\n",
      "An unexpected error occurred: [WinError 183] Cannot create a file when that file already exists: 'Saved_models/R18_E&E\\\\DisplayImage'\n",
      "Entire model saved to: Saved_models/R18_E&E/R18_E&E.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelname=\"R18_E&E\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"Saved_models/{modelname}\") # Will not create parent folders, unlike os.makedirs()\n",
    "    print(f\"Folder created successfully\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating folder: {e}\")    \n",
    "tensorpath=\"runs\\DisplayImage\"\n",
    "copy_directory(tensorpath,f\"Saved_models/{modelname}\")\n",
    "\n",
    "# 7. Save entire model (Less recommended).\n",
    "save_entire_model_path = f\"Saved_models/{modelname}/{modelname}.pth\"\n",
    "torch.save(model, save_entire_model_path)\n",
    "print(f\"Entire model saved to: {save_entire_model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3756\\150428221.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_entire_model = torch.load(f\"Saved_models/{modelname}/simple_model_entire.pth\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Load entire model\n",
    "loaded_entire_model = torch.load(f\"Saved_models/{modelname}/simple_model_entire.pth\")\n",
    "loaded_entire_model = loaded_entire_model.to(device)\n",
    "loaded_entire_model.eval()\n",
    "print(\"Entire model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained VGGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([20, 2])\n"
     ]
    }
   ],
   "source": [
    "test= torchvision.models.vgg16(weights=\"DEFAULT\")\n",
    "test.features[0] = nn.Conv2d(\n",
    "    in_channels=1,  # Change to 1 channel for grayscale\n",
    "    out_channels=64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(1, 1),\n",
    "    padding=(1, 1),\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust the fully connected layer to match the number of output classes\n",
    "num_classes = 2  # Example: MNIST has 10 classes\n",
    "test.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2),\n",
    ")\n",
    "test.avgpool = nn.Identity()\n",
    "\n",
    "print(test)\n",
    "x=torch.randn(20,1,64,64)\n",
    "print(test(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([20, 2])\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.features[0] = nn.Conv2d(\n",
    "    in_channels=1,  # Change to 1 channel for grayscale\n",
    "    out_channels=64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(1, 1),\n",
    "    padding=(1, 1),\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust the fully connected layer to match the number of output classes\n",
    "num_classes = 2  # Example: MNIST has 10 classes\n",
    "model.classifier = nn.Sequential(\n",
    "            nn.Linear(9216, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2),\n",
    ")\n",
    "model.avgpool = nn.Identity()\n",
    "\n",
    "print(model)\n",
    "x=torch.randn(20,1,64,64)\n",
    "print(model(x).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLS_CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
