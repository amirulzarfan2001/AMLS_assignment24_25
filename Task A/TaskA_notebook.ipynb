{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking_GPU_Availabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMD Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard --logdir=\"C:\\Users\\User\\OneDrive - University College London\\UCL Education\\Year 4\\MLS\\Coursework\\AMLS_assignment24_25\\Task A\\runs\\DisplayImage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports, data loading and visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import BreastMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "from tensorboard import program\n",
    "\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from tqdm import tqdm  # For nice progress bar!\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datatsets from BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomAdjustSharpness(sharpness_factor=2,p=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomAdjustSharpness(p=1,sharpness_factor=1.1),\n",
    "    # transforms.RandomEqualize(p=0.3),\n",
    "    # transforms.RandomVerticalFlip(p=0.1),\n",
    "    # transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Mean and standard deviation for grayscale images\n",
    "])\n",
    "#Cannot blindly augment the data to what we want\n",
    "#the classifiers depend on the chape of the tumer hence all the below is good\n",
    "#Might consider increaasing the contrast and brightness to allow easier identificationn\n",
    "#CANCEL contrast... USE histogram equaliser instetad\n",
    "transforms.RandomVerticalFlip()\n",
    "transforms.RandomHorizontalFlip()\n",
    "transforms.RandomRotation(degrees=180)\n",
    "transforms.RandomEqualize(p=1)\n",
    "transforms.RandomAdjustSharpness(p=1,sharpness_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "#loading train,val,test into variables\n",
    "train_data=medmnist.BreastMNIST(split=\"train\",transform=transform)\n",
    "val_data=medmnist.BreastMNIST(split=\"val\",transform=transforms.ToTensor())\n",
    "test_data=medmnist.BreastMNIST(split=\"test\",transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation and class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before oversampling: Counter({1: 399, 0: 147})\n",
      "Batch 1 class distribution: Counter({0: 17, 1: 15})\n",
      "Batch 2 class distribution: Counter({0: 17, 1: 15})\n",
      "Batch 3 class distribution: Counter({0: 19, 1: 13})\n",
      "Total class distribution in the dataloader: Counter({1: 299, 0: 247})\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = medmnist.BreastMNIST(split=\"train\",transform=transform)\n",
    "\n",
    "# Count class occurrences\n",
    "class_counts = Counter(dataset.labels.reshape(-1))\n",
    "print(f\"Class distribution before oversampling: {class_counts}\")\n",
    "\n",
    "# Compute sample weights: Inverse of class frequencies\n",
    "class_weights = 1.0 / torch.tensor([class_counts[cls] for cls in sorted(class_counts.keys())], dtype=torch.float)\n",
    "sample_weights = torch.tensor([class_weights[label] for label in dataset.labels.reshape(-1)], dtype=torch.float)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoader with sampler\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "train_data=dataloader\n",
    "# Check the distribution of classes in a few batches\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1} class distribution: {Counter(labels.reshape(-1).tolist())}\")\n",
    "    if batch_idx == 2:  # Check only the first 3 batches\n",
    "        break\n",
    "\n",
    "# Initialize a Counter for tracking class counts\n",
    "total_class_counts = Counter()\n",
    "\n",
    "# Iterate through the entire dataloader\n",
    "for data, labels in dataloader:\n",
    "    # Flatten the labels and convert them to a list\n",
    "    total_class_counts.update(labels.reshape(-1).tolist())\n",
    "\n",
    "# Print the total distribution of classes\n",
    "print(f\"Total class distribution in the dataloader: {total_class_counts}\")\n",
    "print(len(dataloader)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 52, 0: 12})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels=next(iter(train_loader))\n",
    "x=labels.view(-1)\n",
    "x=x.tolist()\n",
    "print(x)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Example mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(train_data)\n",
    "# print(\"=====================\")\n",
    "# print(val_data)\n",
    "# print(\"=====================\")\n",
    "# print(test_data)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "#visualising the size of the image and its labels\n",
    "one=0\n",
    "zero=0\n",
    "for image , label in train_data:\n",
    "    # print(image.shape)\n",
    "    # print(label)\n",
    "    if label==0:\n",
    "        zero+=1\n",
    "    else:\n",
    "        one+=1\n",
    "print(one)\n",
    "print(zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying images on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location of tensorboard folder\n",
    "folder=\"runs/DisplayImage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1733867044.DESKTOP-3FC1MTH.29016.68\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_folder(folder)\n",
    "#show using dataset on tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "for index in range(100):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1733866663.DESKTOP-3FC1MTH.29016.63\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SummaryWriter.close of <torch.utils.tensorboard.writer.SummaryWriter object at 0x0000022A861F86A0>>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_folder(folder)\n",
    "#show using dataloader with batches\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # create grid of images\n",
    "    img_grid = torchvision.utils.make_grid(data)\n",
    "    # write to tensorboard\n",
    "    writer.add_image(f\"MNIST Example - image batch \", img_grid,batch_idx)\n",
    "    #print(batch_idx)\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label = train_data[1]\n",
    "image_np = img.squeeze()\n",
    "print(image_np.shape)\n",
    "# Plot the image\n",
    "plt.imshow(image_np,cmap=\"gray\")\n",
    "plt.axis('off')  # Hide the axes for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to copy tensor files save\n",
    "def copy_directory(source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Copies all files and folders from a source directory to a destination directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The path to the source directory.\n",
    "        destination_dir (str): The path to the existing destination directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the destination directory exists\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            print(f\"Error: Destination directory '{destination_dir}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Copy the content of source dir to destination directory\n",
    "        shutil.copytree(source_dir, os.path.join(destination_dir, os.path.basename(source_dir)))\n",
    "        print(f\"Successfully copied the content of '{source_dir}' to '{destination_dir}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "    except shutil.Error as e:\n",
    "        print(f\"Error during copy: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#function to clear tensorboard files\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Iterate through all items in the folder\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        try:\n",
    "            # Remove directories\n",
    "            if os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "            # Remove files\n",
    "            else:\n",
    "                os.remove(item_path)\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")\n",
    "    \n",
    "    print(f\"All contents of the folder '{folder_path}' have been cleared.\")\n",
    "    #location of tensorboard folder\n",
    "folder=\"runs/DisplayImage\"\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\n",
    "        Here we define the layers of the network. We create two fully connected layers\n",
    "\n",
    "        Parameters:\n",
    "            input_size: the size of the input, in this case 784 (28x28)\n",
    "            num_classes: the number of classes we want to predict, in this case 2 (0-1)\n",
    "\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "        # # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # # this case 10.\n",
    "        # self.fc1 = nn.Linear(input_size, 50)\n",
    "        # self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "        self.flatten = nn.Flatten() #flattens the input tensors\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x here is the mnist images and we run it through the network that we created above.\n",
    "        Parameters:\n",
    "            x: mnist images\n",
    "        Returns:\n",
    "            out: the output of the network\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(  #same convolution\n",
    "            in_channels=in_channels,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16, \n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \"\"\"\n",
    "        Here we define the layers of the network. We create two fully connected layers\n",
    "\n",
    "        Parameters:\n",
    "            input_size: the size of the input, in this case 784 (28x28)\n",
    "            num_classes: the number of classes we want to predict, in this case 10 (0-9)\n",
    "\n",
    "        \"\"\"\n",
    "        super(NN2, self).__init__()\n",
    "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # this case 10.\n",
    "        self.flatten = nn.Flatten() #flattens the input tensors\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
    "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
    "        I recommend using nn.functional (F)\n",
    "\n",
    "        Parameters:\n",
    "            x: mnist images\n",
    "\n",
    "        Returns:\n",
    "            out: the output of the network\n",
    "        \"\"\"\n",
    "        x=self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    accuracies=[]\n",
    "    batch_loss=0\n",
    "    for batch, (input_data, class_cat) in enumerate(tqdm(dataloader)):\n",
    "        input_data, class_cat = input_data.to(device), class_cat.to(device)\n",
    "\n",
    "        ## Compute prediction error\n",
    "        pred = model(input_data)\n",
    "        class_cat=class_cat.squeeze().long()\n",
    "        loss = loss_fn(pred, class_cat)\n",
    "\n",
    "        ## Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## calculation running loss\n",
    "        loss, current = loss.item(), (batch + 1) * len(input_data)\n",
    "\n",
    "\n",
    "        ##caculating running accuracy\n",
    "        _, predictions = pred.max(1)\n",
    "        num_correct = (predictions == class_cat).sum()\n",
    "        running_train_acc = float(num_correct) / float(input_data.shape[0])\n",
    "        # print(\"model Output>>>>>\")\n",
    "        # print(pred)\n",
    "        # print(\"predictions>>>\")\n",
    "        # print(predictions)\n",
    "        # print(\"num_correct>>>>>\")\n",
    "        # print(num_correct)\n",
    "        # print(\"accuracy>>>>\")\n",
    "        # print(running_train_acc)\n",
    "        # print(data.shape[0])\n",
    "        # print(input_data.shape[0])\n",
    "        accuracies.append(running_train_acc)\n",
    "\n",
    "        ##Plot stuff to tensorboard tensorboard\n",
    "        global step\n",
    "        writer.add_scalar(\"Batch/Training loss\",loss,global_step=step)\n",
    "        writer.add_scalar(\"Batch/Training Accuracy\", running_train_acc, global_step=step)\n",
    "        # global batch_loss\n",
    "        # batch_loss.append(loss)\n",
    "        batch_loss+=loss\n",
    "\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "\n",
    "        #print(f\"loss: {loss:>7f} accuracy: {running_train_acc:>5f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    ## Calculate epoch accuracy\n",
    "    epoch_accuracy=sum(accuracies)/len(accuracies)\n",
    "\n",
    "    ## Getting the average epoch loss\n",
    "    epoch_loss=batch_loss/size\n",
    "    \n",
    "    ## Send it to tensorboard\n",
    "    writer.add_scalars(\"Epoch/Epoch loss\",{'Train':epoch_loss},global_step=epoch)\n",
    "    writer.add_scalars(\"Epoch/Epoch Accuracy\",{\"Train\":epoch_accuracy},global_step=epoch)\n",
    "    \n",
    "def val(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y=y.squeeze().long()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    writer.add_scalars(\"Epoch/Epoch loss\",{'Test':test_loss},global_step=epoch)\n",
    "    writer.add_scalars(\"Epoch/Epoch Accuracy\",{\"Test\":correct},global_step=epoch)\n",
    "    print(f\"val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.squeeze().long()\n",
    "            pred = model(X)\n",
    "            # Collect predictions and true labels\n",
    "            all_labels.append(int(y))\n",
    "            all_predictions.append(int(pred.argmax(1)))\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct /= size\n",
    "\n",
    "    # Build confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_names = [\"False\", \"Positive\"]  # Update as needed for your use case\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()  # Ensure labels fit within figure boundaries\n",
    "    \n",
    "\n",
    "    # Convert plot to image\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.draw()\n",
    "    width, height = fig.canvas.get_width_height()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "    # Log image to \n",
    "    writer.add_image(\"Confusion Matrix\", np.transpose(image, (2, 0, 1)),global_step=1)\n",
    " \n",
    "    # Log accuracy to TensorBoard\n",
    "    writer.add_scalar(\"Test Accuracy\", correct,global_step=1)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%\\n\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "def savemodel(model_name,model,tensor_path):\n",
    "    try:\n",
    "        os.mkdir(\"only_one_folder\") # Will not create parent folders, unlike os.makedirs()\n",
    "        print(f\"Folder created successfully\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating folder: {e}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "#basinc testing for the model\n",
    "model=CNN()\n",
    "x=torch.randn(20,1,28,28)\n",
    "print(model(x).shape)\n",
    "\n",
    "model=NN(input_size=28*28,num_classes=2)\n",
    "x=torch.randn(20,1,28,28)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#batch_size = 64 Note: this was determined when we loaded the data previously\u001b[39;00m\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m(input_size\u001b[38;5;241m=\u001b[39minput_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m## Setting up training and test function\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NN' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "#batch_size = 64 Note: this was determined when we loaded the data previously\n",
    "num_epochs = 100\n",
    "\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "for index, (data,label) in enumerate(train_loader):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "step=0\n",
    "# epoch_loss=[]\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(val_loader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=2, bias=True)\n",
      ")\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch Accuracy_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch Accuracy_Train'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Test: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Test'\n",
      "Failed to delete runs/DisplayImage\\Epoch_Epoch loss_Train: [WinError 5] Access is denied: 'runs/DisplayImage\\\\Epoch_Epoch loss_Train'\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1733980011.DESKTOP-3FC1MTH.3756.27\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n",
      "Model sent to tensorboard\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 59.69it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m     train(train_loader, model, loss_fn, optimizer)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# epoch_loss.append(sum(batch_loss)/len(batch_loss))\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[101], line 232\u001b[0m, in \u001b[0;36mval\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m    230\u001b[0m         y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m    231\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m--> 232\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    233\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    234\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "# Full implementation\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "# for index, (data,label) in enumerate(train_loader):\n",
    "#     data,label=train_data[index]\n",
    "#     writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "step=0\n",
    "# epoch_loss=[]\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(val_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(test_loader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\MLS_CW\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Deleted: runs/DisplayImage\\events.out.tfevents.1734027252.DESKTOP-3FC1MTH.15284.25\n",
      "All contents of the folder 'runs/DisplayImage' have been cleared.\n",
      "Model sent to tensorboard\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.112666 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.443056 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.589005 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.557889 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.524640 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.684350 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.313106 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.664423 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.525142 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.449016 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.323898 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.537226 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 1.318158 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.642013 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.548310 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 25.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.416348 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.472683 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.725782 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.497003 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.795853 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.161892 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.648534 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.532615 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.545995 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.254280 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.950612 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.531007 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.405495 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.345998 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 76.9%, Avg loss: 1.522972 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.933369 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.424148 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.495911 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.830802 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 80.8%, Avg loss: 1.261568 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.680352 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.424624 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.297365 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.364343 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.326686 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.720871 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.587245 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.972578 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.369013 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.354572 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.385766 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.511082 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.342107 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.290450 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.308611 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.539697 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.630441 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.455216 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.471053 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.368343 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.648493 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.449085 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.527815 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.513461 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.451320 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.685519 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.703780 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.395576 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.281939 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.569771 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 1.043518 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.700529 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.866936 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.452594 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.389790 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.288210 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.923136 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.578267 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.641640 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 1.110325 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.861164 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.320412 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.329827 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.731306 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.470696 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.352917 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.926638 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.383920 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.445985 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 1.020864 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.856246 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.518110 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.659246 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.603727 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.412587 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.616350 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.850094 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.481396 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.257237 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 23.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.324750 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.565983 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.340149 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.709374 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.458685 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:00<00:00, 24.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.197173 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27520\\3390447779.py:280: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.9%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomAdjustSharpness(p=1,sharpness_factor=1.1),\n",
    "    # transforms.RandomEqualize(p=0.3),\n",
    "    # transforms.RandomVerticalFlip(p=0.1),\n",
    "    # transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Mean and standard deviation for grayscale images\n",
    "])\n",
    "\n",
    "batch_size=64\n",
    "#loading train,val,test into variables\n",
    "train_data=medmnist.BreastMNIST(split=\"train\",transform=transform)\n",
    "val_data=medmnist.BreastMNIST(split=\"val\",transform=transforms.ToTensor())\n",
    "test_data=medmnist.BreastMNIST(split=\"test\",transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28*28\n",
    "num_classes = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "#Freeze the model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad=False\n",
    "\n",
    "model.conv1 = nn.Conv2d(\n",
    "    in_channels=1,  # Change to 1 channel for grayscale\n",
    "    out_channels=64,\n",
    "    kernel_size=(7, 7),\n",
    "    stride=(2, 2),\n",
    "    padding=(3, 3),\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# Optionally copy pretrained weights and average across RGB channels\n",
    "with torch.no_grad():\n",
    "    pretrained_weights = model.conv1.weight  # Original weights for RGB\n",
    "    model.conv1.weight.copy_(torch.mean(pretrained_weights, dim=1, keepdim=True))\n",
    "\n",
    "# Adjust the fully connected layer to match the number of output classes\n",
    "num_classes = 2  # Example: MNIST has 10 classes\n",
    "model.fc = nn.Sequential(\n",
    "            nn.Linear(512, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2),\n",
    ")\n",
    "model.avgpool = nn.Identity()\n",
    "\n",
    "print(model)\n",
    "\n",
    "## Setting up training and test function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Move model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "##clear tensorboard folder\n",
    "clear_folder(folder)\n",
    "writer = SummaryWriter(f\"runs/DisplayImage\")\n",
    "\n",
    "#show using dataset on tensorboard\n",
    "for index, (data,label) in enumerate(train_loader):\n",
    "    data,label=train_data[index]\n",
    "    writer.add_image(\"mnist_images\", data,index)\n",
    "\n",
    "# Visualize model in TensorBoard\n",
    "example_img, labels = next(iter(train_loader))\n",
    "#example_img=example_img[0]\n",
    "writer.add_graph(model,example_img.to(device))\n",
    "print(\"Model sent to tensorboard\")\n",
    "\n",
    "\n",
    "step=0\n",
    "# epoch_loss=[]\n",
    "for t in range(num_epochs):\n",
    "    epoch=t\n",
    "    # batch_loss=[]\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    # writer.add_scalar(\"Epoch Training loss\",epoch_loss[t],global_step=t)\n",
    "    val(val_loader, model, loss_fn)\n",
    "\n",
    "test(test_loader,model)\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save current model with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating folder: [WinError 183] Cannot create a file when that file already exists: 'Saved_models/R18_E&E'\n",
      "An unexpected error occurred: [WinError 183] Cannot create a file when that file already exists: 'Saved_models/R18_E&E\\\\DisplayImage'\n",
      "Entire model saved to: Saved_models/R18_E&E/R18_E&E.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelname=\"R18_E&E\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"Saved_models/{modelname}\") # Will not create parent folders, unlike os.makedirs()\n",
    "    print(f\"Folder created successfully\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating folder: {e}\")    \n",
    "tensorpath=\"runs\\DisplayImage\"\n",
    "copy_directory(tensorpath,f\"Saved_models/{modelname}\")\n",
    "\n",
    "# 7. Save entire model (Less recommended).\n",
    "save_entire_model_path = f\"Saved_models/{modelname}/{modelname}.pth\"\n",
    "torch.save(model, save_entire_model_path)\n",
    "print(f\"Entire model saved to: {save_entire_model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3756\\150428221.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_entire_model = torch.load(f\"Saved_models/{modelname}/simple_model_entire.pth\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Load entire model\n",
    "loaded_entire_model = torch.load(f\"Saved_models/{modelname}/simple_model_entire.pth\")\n",
    "loaded_entire_model = loaded_entire_model.to(device)\n",
    "loaded_entire_model.eval()\n",
    "print(\"Entire model loaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLS_CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
